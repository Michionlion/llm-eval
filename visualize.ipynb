{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy plotly nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d827777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance sweep viz (interactive Plotly) + per-run variance + outlier flagging\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "## 0) Plotly renderer (VSCode)\n",
    "pio.renderers.default = \"vscode\" if os.environ.get(\"VSCODE_PID\") else \"notebook_connected\"\n",
    "\n",
    "\n",
    "## 1) Paths / load\n",
    "DATA_DIR = Path(\".\")  # change to Path(\"...\") if your CSVs live elsewhere\n",
    "\n",
    "LOAD_TIMES_CSV = DATA_DIR / \"load_times.csv\"\n",
    "REQUESTS_CSV   = DATA_DIR / \"requests.csv\"\n",
    "SUMMARY_CSV    = DATA_DIR / \"summary.csv\"\n",
    "\n",
    "load_df = pd.read_csv(LOAD_TIMES_CSV)\n",
    "req_df  = pd.read_csv(REQUESTS_CSV)\n",
    "sum_df  = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "for df in (load_df, req_df, sum_df):\n",
    "    if \"timestamp_utc\" in df.columns:\n",
    "        df[\"timestamp_utc\"] = pd.to_datetime(df[\"timestamp_utc\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "\n",
    "## 2) Per-run derived metrics (stream runs)\n",
    "stream = req_df[\n",
    "    (~req_df[\"warmup_run\"]) &\n",
    "    (req_df[\"run_mode\"] == \"stream\") &\n",
    "    (req_df[\"ok\"] == True)\n",
    "].copy()\n",
    "\n",
    "stream[\"decode_time_s\"] = stream[\"wall_s\"] - stream[\"ttft_s\"]\n",
    "stream[\"prefill_eff_tps\"] = stream[\"prompt_tokens\"] / stream[\"ttft_s\"].replace(0, np.nan)\n",
    "stream[\"decode_eff_tps\"]  = stream[\"completion_tokens\"] / stream[\"decode_time_s\"].replace(0, np.nan)\n",
    "stream[\"est_wall_s\"] = stream[\"ttft_s\"] + (stream[\"max_output_tokens\"] / stream[\"decode_eff_tps\"].replace(0, np.nan))\n",
    "\n",
    "stream[\"early_stop\"] = stream[\"completion_tokens\"] < 0.9 * stream[\"max_output_tokens\"]\n",
    "\n",
    "stream = stream[\n",
    "    np.isfinite(stream[\"ttft_s\"]) &\n",
    "    np.isfinite(stream[\"prefill_eff_tps\"]) &\n",
    "    np.isfinite(stream[\"decode_eff_tps\"]) &\n",
    "    np.isfinite(stream[\"est_wall_s\"]) &\n",
    "    (stream[\"ttft_s\"] > 0) &\n",
    "    (stream[\"decode_time_s\"] > 0)\n",
    "].copy()\n",
    "\n",
    "agg = (\n",
    "    stream\n",
    "    .groupby([\"model\", \"target_prompt_tokens\"], as_index=False)\n",
    "    .agg(\n",
    "        n=(\"idx\", \"size\"),\n",
    "        ttft_mean=(\"ttft_s\", \"mean\"),\n",
    "        ttft_std=(\"ttft_s\", \"std\"),\n",
    "        prefill_mean=(\"prefill_eff_tps\", \"mean\"),\n",
    "        prefill_std=(\"prefill_eff_tps\", \"std\"),\n",
    "        decode_mean=(\"decode_eff_tps\", \"mean\"),\n",
    "        decode_std=(\"decode_eff_tps\", \"std\"),\n",
    "        est_wall_mean=(\"est_wall_s\", \"mean\"),\n",
    "        est_wall_std=(\"est_wall_s\", \"std\"),\n",
    "        completion_tokens_min=(\"completion_tokens\", \"min\"),\n",
    "        completion_tokens_med=(\"completion_tokens\", \"median\"),\n",
    "        completion_tokens_max=(\"completion_tokens\", \"max\"),\n",
    "        early_stop_frac=(\"early_stop\", \"mean\"),\n",
    "    )\n",
    "    .sort_values([\"model\", \"target_prompt_tokens\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "for c in [\"ttft_std\", \"prefill_std\", \"decode_std\", \"est_wall_std\"]:\n",
    "    agg[c] = agg[c].fillna(0.0)\n",
    "\n",
    "\n",
    "## 3) Outlier detection (stream vs nonstream wall mismatches)\n",
    "nonstream = req_df[\n",
    "    (~req_df[\"warmup_run\"]) &\n",
    "    (req_df[\"run_mode\"] == \"nonstream\") &\n",
    "    (req_df[\"ok\"] == True)\n",
    "].copy()\n",
    "\n",
    "paired = stream.merge(\n",
    "    nonstream[[\"job_id\", \"idx\", \"wall_s\"]],\n",
    "    on=[\"job_id\", \"idx\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_stream\", \"_nonstream\"),\n",
    ")\n",
    "\n",
    "paired[\"wall_diff_s\"] = paired[\"wall_s_stream\"] - paired[\"wall_s_nonstream\"]\n",
    "timing_outliers = paired[\n",
    "    paired[\"wall_s_nonstream\"].notna() &\n",
    "    (paired[\"wall_diff_s\"].abs() > 5)\n",
    "].copy()\n",
    "\n",
    "\n",
    "## 4) Shared colors\n",
    "models = sorted(agg[\"model\"].unique())\n",
    "palette = (\n",
    "    px.colors.qualitative.Alphabet\n",
    "    + px.colors.qualitative.Dark24\n",
    "    + px.colors.qualitative.Light24\n",
    ")\n",
    "base_color_map = {m: palette[i % len(palette)] for i, m in enumerate(models)}\n",
    "\n",
    "def hex_to_rgba(hex_color: str, alpha: float) -> str:\n",
    "    hc = hex_color.lstrip(\"#\")\n",
    "    if len(hc) != 6:\n",
    "        return hex_color\n",
    "    r = int(hc[0:2], 16)\n",
    "    g = int(hc[2:4], 16)\n",
    "    b = int(hc[4:6], 16)\n",
    "    return f\"rgba({r},{g},{b},{alpha})\"\n",
    "\n",
    "LINE_ALPHA = 0.70\n",
    "FILL_ALPHA = 0.20\n",
    "POINT_ALPHA = 0.35\n",
    "\n",
    "\n",
    "## 5) Tick helpers\n",
    "def _nice_step(raw_step: float) -> float:\n",
    "    if not np.isfinite(raw_step) or raw_step <= 0:\n",
    "        return 1.0\n",
    "    exp = np.floor(np.log10(raw_step))\n",
    "    base = raw_step / (10 ** exp)\n",
    "    if base <= 1:\n",
    "        nice = 1\n",
    "    elif base <= 2:\n",
    "        nice = 2\n",
    "    elif base <= 5:\n",
    "        nice = 5\n",
    "    else:\n",
    "        nice = 10\n",
    "    return nice * (10 ** exp)\n",
    "\n",
    "def dense_y_ticks(fig: go.Figure, y_min: float, y_max: float, target_ticks: int = 14):\n",
    "    if not (np.isfinite(y_min) and np.isfinite(y_max)) or y_min == y_max:\n",
    "        return fig\n",
    "    span = y_max - y_min\n",
    "    raw_step = span / max(2, target_ticks - 1)\n",
    "    dtick = _nice_step(raw_step)\n",
    "    y0 = np.floor(y_min / dtick) * dtick\n",
    "    y1 = np.ceil(y_max / dtick) * dtick\n",
    "    fig.update_yaxes(tickmode=\"linear\", tick0=y0, dtick=dtick, range=[y0, y1])\n",
    "    return fig\n",
    "\n",
    "def _range(df: pd.DataFrame, col: str) -> tuple[float, float]:\n",
    "    v = pd.to_numeric(df[col], errors=\"coerce\").dropna().values\n",
    "    if v.size == 0:\n",
    "        return (0.0, 1.0)\n",
    "    return (float(v.min()), float(v.max()))\n",
    "\n",
    "\n",
    "## 6) Plot primitives\n",
    "def mean_line_with_band_and_errors(\n",
    "    agg_df: pd.DataFrame,\n",
    "    run_df: pd.DataFrame,\n",
    "    y_mean_col: str,\n",
    "    y_std_col: str,\n",
    "    run_y_col: str,\n",
    "    title: str,\n",
    "    y_label: str,\n",
    "    show_band: bool = True,\n",
    "    show_errorbars: bool = True,\n",
    "    show_runs: bool = True,\n",
    ") -> go.Figure:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if show_runs:\n",
    "        for m in models:\n",
    "            d = run_df[run_df[\"model\"] == m].sort_values(\"target_prompt_tokens\")\n",
    "            if d.empty:\n",
    "                continue\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=d[\"target_prompt_tokens\"],\n",
    "                    y=d[run_y_col],\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"{m} (runs)\",\n",
    "                    legendgroup=m,\n",
    "                    showlegend=False,\n",
    "                    marker=dict(\n",
    "                        color=hex_to_rgba(base_color_map[m], POINT_ALPHA),\n",
    "                        size=7,\n",
    "                    ),\n",
    "                    hovertemplate=(\n",
    "                        f\"Model: {m}<br>\"\n",
    "                        \"Prompt: %{x}<br>\"\n",
    "                        f\"{run_y_col}: %{{y:.4g}}<br>\"\n",
    "                        \"job_id: %{customdata[0]}<br>\"\n",
    "                        \"idx: %{customdata[1]}<extra></extra>\"\n",
    "                    ),\n",
    "                    customdata=np.stack([d[\"job_id\"].astype(str), d[\"idx\"].astype(str)], axis=1),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    for m in models:\n",
    "        g = agg_df[agg_df[\"model\"] == m].sort_values(\"target_prompt_tokens\")\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        x = g[\"target_prompt_tokens\"].values\n",
    "        y = g[y_mean_col].values\n",
    "        s = g[y_std_col].values\n",
    "        color_line = hex_to_rgba(base_color_map[m], LINE_ALPHA)\n",
    "\n",
    "        if show_band:\n",
    "            upper = y + s\n",
    "            lower = y - s\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x,\n",
    "                    y=upper,\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=0),\n",
    "                    name=f\"{m} (+1σ)\",\n",
    "                    legendgroup=m,\n",
    "                    showlegend=False,\n",
    "                    hoverinfo=\"skip\",\n",
    "                )\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x,\n",
    "                    y=lower,\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=0),\n",
    "                    fill=\"tonexty\",\n",
    "                    fillcolor=hex_to_rgba(base_color_map[m], FILL_ALPHA),\n",
    "                    name=f\"{m} (±1σ)\",\n",
    "                    legendgroup=m,\n",
    "                    showlegend=False,\n",
    "                    hoverinfo=\"skip\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        err = dict(type=\"data\", array=s, visible=True) if show_errorbars else None\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode=\"lines+markers\",\n",
    "                name=m,\n",
    "                legendgroup=m,\n",
    "                line=dict(color=color_line, width=2),\n",
    "                marker=dict(size=8, color=color_line),\n",
    "                error_y=err,\n",
    "                hovertemplate=(\n",
    "                    f\"Model: {m}<br>\"\n",
    "                    \"Prompt: %{x}<br>\"\n",
    "                    f\"{y_mean_col}: %{{y:.4g}}<br>\"\n",
    "                    f\"{y_std_col}: %{{customdata[0]:.4g}}<br>\"\n",
    "                    \"n: %{customdata[1]}<extra></extra>\"\n",
    "                ),\n",
    "                customdata=np.stack([s, g[\"n\"].values], axis=1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "        xaxis_title=\"Target prompt tokens\",\n",
    "        yaxis_title=y_label,\n",
    "        margin=dict(l=75, r=25, t=60, b=60),\n",
    "        legend_title_text=\"Model\",\n",
    "    )\n",
    "    fig.update_xaxes(tickmode=\"array\", tickvals=sorted(agg_df[\"target_prompt_tokens\"].unique()))\n",
    "\n",
    "    y_min, y_max = _range(agg_df, y_mean_col)\n",
    "    pad = 0.05 * (y_max - y_min) if y_max > y_min else 1.0\n",
    "    dense_y_ticks(fig, y_min - pad, y_max + pad, target_ticks=16)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def heatmap_mean(\n",
    "    agg_df: pd.DataFrame,\n",
    "    value_col: str,\n",
    "    title: str,\n",
    "    z_fmt: str = \".4g\"\n",
    ") -> go.Figure:\n",
    "    piv = agg_df.pivot_table(index=\"model\", columns=\"target_prompt_tokens\", values=value_col, aggfunc=\"mean\")\n",
    "    piv = piv.reindex(index=models)\n",
    "    piv = piv.sort_index(axis=1)\n",
    "\n",
    "    x = [int(c) for c in piv.columns]\n",
    "    y = piv.index.tolist()\n",
    "    z = piv.values\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            colorbar=dict(title=value_col),\n",
    "            hovertemplate=(\n",
    "                \"Model: %{y}<br>\"\n",
    "                \"Prompt tokens: %{x}<br>\"\n",
    "                f\"{value_col}: %{{z:{z_fmt}}}<extra></extra>\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        template=\"plotly_white\",\n",
    "        xaxis_title=\"Target prompt tokens\",\n",
    "        yaxis_title=\"Model\",\n",
    "        margin=dict(l=180, r=25, t=60, b=60),\n",
    "    )\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_xaxes(tickmode=\"array\", tickvals=x)\n",
    "    return fig\n",
    "\n",
    "\n",
    "## Shared subset for 4096-target products\n",
    "df_4096 = agg[agg[\"target_prompt_tokens\"] == 4096].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe144d",
   "metadata": {},
   "source": [
    "## Figure 1: TTFT vs Prompt Length\n",
    "Mean TTFT by model across prompt sizes, with per-run points, ±1σ bands, and error bars. Use this to compare startup responsiveness under increasing context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: TTFT vs prompt length\n",
    "fig_ttft = mean_line_with_band_and_errors(\n",
    "    agg_df=agg,\n",
    "    run_df=stream,\n",
    "    y_mean_col=\"ttft_mean\",\n",
    "    y_std_col=\"ttft_std\",\n",
    "    run_y_col=\"ttft_s\",\n",
    "    title=\"TTFT vs prompt length (mean ±1σ, per-run dots)\",\n",
    "    y_label=\"TTFT (seconds)\",\n",
    "    show_band=True,\n",
    "    show_errorbars=True,\n",
    "    show_runs=True,\n",
    ")\n",
    "fig_ttft.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ca24d",
   "metadata": {},
   "source": [
    "## Figure 2: Prefill Throughput vs Prompt Length\n",
    "Mean prefill throughput ($tokens/s$) with variability overlays. This highlights how efficiently each model processes input context before decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Prefill throughput vs prompt length\n",
    "fig_prefill = mean_line_with_band_and_errors(\n",
    "    agg_df=agg,\n",
    "    run_df=stream,\n",
    "    y_mean_col=\"prefill_mean\",\n",
    "    y_std_col=\"prefill_std\",\n",
    "    run_y_col=\"prefill_eff_tps\",\n",
    "    title=\"Prefill throughput vs prompt length (mean ±1σ, per-run dots)\",\n",
    "    y_label=\"Prefill throughput (tokens/sec)\",\n",
    "    show_band=True,\n",
    "    show_errorbars=True,\n",
    "    show_runs=True,\n",
    ")\n",
    "fig_prefill.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd13aa",
   "metadata": {},
   "source": [
    "## Figure 3: Decode Throughput vs Prompt Length\n",
    "Mean decode throughput ($tokens/s$) with per-run scatter and ±1σ context. Use this for generation-speed comparisons across context sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Decode throughput vs prompt length\n",
    "fig_decode = mean_line_with_band_and_errors(\n",
    "    agg_df=agg,\n",
    "    run_df=stream,\n",
    "    y_mean_col=\"decode_mean\",\n",
    "    y_std_col=\"decode_std\",\n",
    "    run_y_col=\"decode_eff_tps\",\n",
    "    title=\"Decode throughput vs prompt length (mean ±1σ, per-run dots)\",\n",
    "    y_label=\"Decode throughput (tokens/sec)\",\n",
    "    show_band=True,\n",
    "    show_errorbars=True,\n",
    "    show_runs=True,\n",
    ")\n",
    "fig_decode.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a4b1c",
   "metadata": {},
   "source": [
    "## Figure 4: Estimated Wall Time vs Prompt Length\n",
    "Estimated completion time defined as $TTFT + \\frac{max\\_output\\_tokens}{decode\\_throughput}$. Useful for end-to-end latency planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ea1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Estimated wall time vs prompt length\n",
    "fig_est_wall = mean_line_with_band_and_errors(\n",
    "    agg_df=agg,\n",
    "    run_df=stream,\n",
    "    y_mean_col=\"est_wall_mean\",\n",
    "    y_std_col=\"est_wall_std\",\n",
    "    run_y_col=\"est_wall_s\",\n",
    "    title=\"Estimated wall time vs prompt length (TTFT + max_output/decode) (mean ±1σ, per-run dots)\",\n",
    "    y_label=\"Estimated wall time (seconds)\",\n",
    "    show_band=True,\n",
    "    show_errorbars=True,\n",
    "    show_runs=True,\n",
    ")\n",
    "fig_est_wall.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e748181",
   "metadata": {},
   "source": [
    "## Figure 5: Prefill Throughput Heatmap (Mean)\n",
    "Heatmap of average prefill throughput by model and prompt length for quick ranking of context-ingestion performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Prefill throughput heatmap (mean)\n",
    "fig_heat_prefill = heatmap_mean(agg, \"prefill_mean\", \"Prefill throughput heatmap (mean tokens/sec)\")\n",
    "fig_heat_prefill.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a7f38",
   "metadata": {},
   "source": [
    "## Figure 6: Decode Throughput Heatmap (Mean)\n",
    "Heatmap of average decode throughput by model and prompt length to compare generation speed at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad84da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Decode throughput heatmap (mean)\n",
    "fig_heat_decode = heatmap_mean(agg, \"decode_mean\", \"Decode throughput heatmap (mean tokens/sec)\")\n",
    "fig_heat_decode.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05681b0",
   "metadata": {},
   "source": [
    "## Figure 7: Prefill Throughput Heatmap (Std Dev)\n",
    "Variability heatmap for prefill throughput. Larger values indicate less stable performance across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce60547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Prefill throughput heatmap (std dev)\n",
    "fig_heat_prefill_std = heatmap_mean(agg, \"prefill_std\", \"Prefill throughput heatmap (std dev tokens/sec)\")\n",
    "fig_heat_prefill_std.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310da8b6",
   "metadata": {},
   "source": [
    "## Figure 8: Decode Throughput Heatmap (Std Dev)\n",
    "Variability heatmap for decode throughput to identify unstable generation behavior by model and context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Decode throughput heatmap (std dev)\n",
    "fig_heat_decode_std = heatmap_mean(agg, \"decode_std\", \"Decode throughput heatmap (std dev tokens/sec)\")\n",
    "fig_heat_decode_std.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025fdadd",
   "metadata": {},
   "source": [
    "## Figure 9: Tradeoff at 4096 Tokens\n",
    "Scatter plot of TTFT versus decode throughput at 4096 prompt tokens with ±1σ error bars, showing latency/speed tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e14cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Tradeoff @ 4096 prompt tokens\n",
    "fig_tradeoff_4096 = px.scatter(\n",
    "    df_4096,\n",
    "    x=\"ttft_mean\",\n",
    "    y=\"decode_mean\",\n",
    "    color=\"model\",\n",
    "    color_discrete_map=base_color_map,\n",
    "    error_x=\"ttft_std\",\n",
    "    error_y=\"decode_std\",\n",
    "    hover_data={\n",
    "        \"model\": True,\n",
    "        \"n\": True,\n",
    "        \"ttft_mean\": \":.4g\",\n",
    "        \"ttft_std\": \":.4g\",\n",
    "        \"prefill_mean\": \":.4g\",\n",
    "        \"decode_mean\": \":.4g\",\n",
    "        \"decode_std\": \":.4g\",\n",
    "        \"est_wall_mean\": \":.4g\",\n",
    "        \"early_stop_frac\": \":.3f\",\n",
    "    },\n",
    "    title=\"Tradeoff @ 4096 prompt tokens: TTFT vs decode (error bars = ±1σ)\",\n",
    ")\n",
    "fig_tradeoff_4096.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    xaxis_title=\"TTFT mean (s) @ 4096\",\n",
    "    yaxis_title=\"Decode throughput mean (tokens/sec) @ 4096\",\n",
    "    margin=dict(l=75, r=25, t=60, b=60),\n",
    ")\n",
    "dense_y_ticks(fig_tradeoff_4096, *_range(df_4096, \"decode_mean\"), target_ticks=14)\n",
    "fig_tradeoff_4096.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b7bda",
   "metadata": {},
   "source": [
    "## Figure 10: Stream vs Non-Stream Wall-Time Outliers\n",
    "Highlights runs where $|wall_{stream} - wall_{nonstream}| > 5s$. Use this to investigate suspicious timing divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Outliers for stream vs nonstream wall mismatch\n",
    "if not timing_outliers.empty:\n",
    "    fig_wall_mismatch = px.scatter(\n",
    "        timing_outliers,\n",
    "        x=\"target_prompt_tokens\",\n",
    "        y=\"wall_diff_s\",\n",
    "        color=\"model\",\n",
    "        color_discrete_map=base_color_map,\n",
    "        symbol=\"model\",\n",
    "        hover_data={\n",
    "            \"job_id\": True,\n",
    "            \"idx\": True,\n",
    "            \"wall_s_stream\": \":.4g\",\n",
    "            \"wall_s_nonstream\": \":.4g\",\n",
    "            \"wall_diff_s\": \":.4g\",\n",
    "            \"ttft_s\": \":.4g\",\n",
    "            \"completion_tokens\": True,\n",
    "            \"max_output_tokens\": True,\n",
    "            \"early_stop\": True,\n",
    "        },\n",
    "        title=\"Outliers: stream vs nonstream wall-time mismatch (wall_stream - wall_nonstream)\",\n",
    "    )\n",
    "    fig_wall_mismatch.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        xaxis_title=\"Target prompt tokens\",\n",
    "        yaxis_title=\"Wall time difference (seconds)\",\n",
    "        margin=dict(l=75, r=25, t=60, b=60),\n",
    "    )\n",
    "    dense_y_ticks(fig_wall_mismatch, *_range(timing_outliers, \"wall_diff_s\"), target_ticks=12)\n",
    "    fig_wall_mismatch.show()\n",
    "else:\n",
    "    print(\"No stream/nonstream wall-time outliers above threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95e084",
   "metadata": {},
   "source": [
    "## Product 1: Leaderboard Table at 4096\n",
    "Sorted table of models by estimated wall time at 4096 prompt tokens, including TTFT/prefill/decode means, std dev, and early-stop fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product: Leaderboard table @ 4096\n",
    "leader_4096 = (\n",
    "    df_4096[[\"model\", \"ttft_mean\", \"ttft_std\", \"prefill_mean\", \"prefill_std\", \"decode_mean\", \"decode_std\", \"est_wall_mean\", \"est_wall_std\", \"early_stop_frac\", \"n\"]]\n",
    "    .sort_values(\"est_wall_mean\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "display(leader_4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc13f78",
   "metadata": {},
   "source": [
    "## Product 2: Early-Stop Runs\n",
    "Lists runs where completion tokens are below the early-stop threshold, with timing and throughput fields for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38975bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product: Runs with early stops\n",
    "early_stop_runs = stream[stream[\"early_stop\"]][\n",
    "    [\"job_id\", \"model\", \"target_prompt_tokens\", \"idx\", \"completion_tokens\", \"max_output_tokens\", \"wall_s\", \"ttft_s\", \"decode_eff_tps\", \"prefill_eff_tps\"]\n",
    "] .sort_values([\"model\", \"target_prompt_tokens\", \"idx\"])\n",
    "if not early_stop_runs.empty:\n",
    "    display(early_stop_runs)\n",
    "else:\n",
    "    print(\"No early-stop runs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67111b0c",
   "metadata": {},
   "source": [
    "## Product 3: Timing Mismatch Table\n",
    "Detailed table of stream/non-stream wall-time mismatches for direct inspection and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbeb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product: Stream vs nonstream mismatch table\n",
    "if not timing_outliers.empty:\n",
    "    display(\n",
    "        timing_outliers[\n",
    "            [\"job_id\", \"model\", \"target_prompt_tokens\", \"idx\", \"wall_s_stream\", \"wall_s_nonstream\", \"wall_diff_s\", \"ttft_s\", \"completion_tokens\", \"max_output_tokens\", \"early_stop\"]\n",
    "        ].sort_values(\"wall_diff_s\")\n",
    "    )\n",
    "else:\n",
    "    print(\"No timing mismatches above threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2aae4",
   "metadata": {},
   "source": [
    "## Product 4: Warmup Bookkeeping Reminder\n",
    "Prints warmup row count and success rate to help interpret warmup-related anomalies in benchmark logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06807af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product: Warmup bookkeeping reminder\n",
    "warmup_rows = req_df[req_df[\"warmup_run\"]]\n",
    "if not warmup_rows.empty:\n",
    "    print(f\"Note: warmup_run rows in requests.csv: {len(warmup_rows)}; ok-rate={warmup_rows['ok'].mean():.3f} (often 0.0 due to harness bookkeeping/errors).\")\n",
    "else:\n",
    "    print(\"No warmup rows found in requests.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
